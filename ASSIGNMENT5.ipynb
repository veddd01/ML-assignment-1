{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b7fc44",
   "metadata": {},
   "source": [
    "Q1 (Based on Step-by-Step Implementation of Ridge Regression using  Gradient  \n",
    "Descent Optimization)  \n",
    "Generate a dataset with atleast seven highly correlated columns and a target variable. Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge regression cost function is minimum and R2_score is maximum.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea8733f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=1e-05, Lambda=1e-15, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=1e-10, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=1e-05, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=0.001, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=0.1, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=1, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=10, Cost=91.0104, R2=0.0005\n",
      "LR=1e-05, Lambda=20, Cost=91.0105, R2=0.0005\n",
      "LR=0.0001, Lambda=1e-15, Cost=90.6244, R2=0.0047\n",
      "LR=0.0001, Lambda=1e-10, Cost=90.6244, R2=0.0047\n",
      "LR=0.0001, Lambda=1e-05, Cost=90.6244, R2=0.0047\n",
      "LR=0.0001, Lambda=0.001, Cost=90.6244, R2=0.0047\n",
      "LR=0.0001, Lambda=0.1, Cost=90.6244, R2=0.0047\n",
      "LR=0.0001, Lambda=1, Cost=90.6245, R2=0.0047\n",
      "LR=0.0001, Lambda=10, Cost=90.6256, R2=0.0047\n",
      "LR=0.0001, Lambda=20, Cost=90.6267, R2=0.0047\n",
      "LR=0.001, Lambda=1e-15, Cost=89.9430, R2=0.0122\n",
      "LR=0.001, Lambda=1e-10, Cost=89.9430, R2=0.0122\n",
      "LR=0.001, Lambda=1e-05, Cost=89.9430, R2=0.0122\n",
      "LR=0.001, Lambda=0.001, Cost=89.9430, R2=0.0122\n",
      "LR=0.001, Lambda=0.1, Cost=89.9431, R2=0.0122\n",
      "LR=0.001, Lambda=1, Cost=89.9440, R2=0.0122\n",
      "LR=0.001, Lambda=10, Cost=89.9531, R2=0.0121\n",
      "LR=0.001, Lambda=20, Cost=89.9631, R2=0.0121\n",
      "LR=0.01, Lambda=1e-15, Cost=89.9091, R2=0.0125\n",
      "LR=0.01, Lambda=1e-10, Cost=89.9091, R2=0.0125\n",
      "LR=0.01, Lambda=1e-05, Cost=89.9091, R2=0.0125\n",
      "LR=0.01, Lambda=0.001, Cost=89.9091, R2=0.0125\n",
      "LR=0.01, Lambda=0.1, Cost=89.9092, R2=0.0125\n",
      "LR=0.01, Lambda=1, Cost=89.9102, R2=0.0125\n",
      "LR=0.01, Lambda=10, Cost=89.9205, R2=0.0125\n",
      "LR=0.01, Lambda=20, Cost=89.9319, R2=0.0125\n",
      "\n",
      " Best Parameters Found:\n",
      "{'LearningRate': 0.01, 'lambda': 1e-15, 'cost': np.float64(89.9090669672518), 'R2': 0.012548985393457723}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=500, n_features=7, noise=10, effective_rank=1, random_state=42\n",
    ")\n",
    "X = np.hstack([X, X[:, [0]] * 0.9 + np.random.randn(500, 1) * 0.01])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "class RidgeRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, lambda_=1.0, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros((n_features, 1))\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            error = y_pred - y\n",
    "\n",
    "            dw = (1 / n_samples) * (np.dot(X.T, error) + self.lambda_ * self.weights)\n",
    "            db = (1 / n_samples) * np.sum(error)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            cost = self.cost(X, y)\n",
    "            if np.isnan(cost) or np.isinf(cost):\n",
    "                print(f\"Diverged (NaN/Inf) at iter {i} for LR={self.learning_rate}, λ={self.lambda_}\")\n",
    "                break\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def cost(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = self.predict(X)\n",
    "        if np.any(np.isnan(y_pred)):\n",
    "            return np.nan\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        ridge_penalty = (self.lambda_ / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
    "        return mse + ridge_penalty\n",
    "\n",
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01] \n",
    "lambdas = [1e-15, 1e-10, 1e-5, 0.001, 0.1, 1, 10, 20]\n",
    "\n",
    "best_r2 = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lam in lambdas:\n",
    "        model = RidgeRegressionGD(learning_rate=lr, lambda_=lam, n_iters=2000)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        if np.any(np.isnan(y_pred)) or np.any(np.isnan(model.weights)):\n",
    "            print(f\"Skipped LR={lr}, Lambda={lam} due to NaN values\")\n",
    "            continue\n",
    "\n",
    "        cost = model.cost(X, y)\n",
    "        if np.isnan(cost):\n",
    "            continue\n",
    "\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        if np.isnan(r2):\n",
    "            continue\n",
    "\n",
    "        print(f\"LR={lr}, Lambda={lam}, Cost={cost:.4f}, R2={r2:.4f}\")\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_params = {'LearningRate': lr, 'lambda': lam, 'cost': cost, 'R2': r2}\n",
    "\n",
    "print(\"\\n Best Parameters Found:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a401645",
   "metadata": {},
   "source": [
    "Q2 Load the Hitters dataset from the following link \n",
    "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing  \n",
    "(a)\tPre-process the data (null values, noise, categorical to numerical encoding)  \n",
    "(b)\tSeparate input and output features and perform scaling  \n",
    "(c)\tFit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use regularization parameter as 0.5748) regression function on the dataset.  \n",
    "(d)\tEvaluate the performance of each trained model on test set. Which model performs the best and Why?  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39d9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully\n",
      "Shape: (322, 20)\n",
      "First 5 rows:\n",
      "    AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
      "0    293    66      1    30   29     14      1     293     66       1     30   \n",
      "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
      "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
      "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
      "4    321    87     10    39   42     30      2     396    101      12     48   \n",
      "\n",
      "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
      "0    29      14      A        E      446       33      20     NaN         A  \n",
      "1   414     375      N        W      632       43      10   475.0         N  \n",
      "2   266     263      A        W      880       82      14   480.0         A  \n",
      "3   838     354      N        E      200       11       3   500.0         N  \n",
      "4    46      33      N        E      805       40       4    91.5         N  \n",
      "\n",
      "Missing values before:\n",
      " AtBat         0\n",
      "Hits          0\n",
      "HmRun         0\n",
      "Runs          0\n",
      "RBI           0\n",
      "Walks         0\n",
      "Years         0\n",
      "CAtBat        0\n",
      "CHits         0\n",
      "CHmRun        0\n",
      "CRuns         0\n",
      "CRBI          0\n",
      "CWalks        0\n",
      "League        0\n",
      "Division      0\n",
      "PutOuts       0\n",
      "Assists       0\n",
      "Errors        0\n",
      "Salary       59\n",
      "NewLeague     0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      " AtBat        0\n",
      "Hits         0\n",
      "HmRun        0\n",
      "Runs         0\n",
      "RBI          0\n",
      "Walks        0\n",
      "Years        0\n",
      "CAtBat       0\n",
      "CHits        0\n",
      "CHmRun       0\n",
      "CRuns        0\n",
      "CRBI         0\n",
      "CWalks       0\n",
      "League       0\n",
      "Division     0\n",
      "PutOuts      0\n",
      "Assists      0\n",
      "Errors       0\n",
      "Salary       0\n",
      "NewLeague    0\n",
      "dtype: int64\n",
      "\n",
      "Categorical Columns: ['League', 'Division', 'NewLeague']\n",
      "\n",
      "Linear Regression Performance:\n",
      "Mean Squared Error: 128284.3455\n",
      "R2 Score: 0.2907\n",
      "\n",
      "Ridge Regression Performance:\n",
      "Mean Squared Error: 126603.9026\n",
      "R2 Score: 0.3000\n",
      "\n",
      "Lasso Regression Performance:\n",
      "Mean Squared Error: 126739.5690\n",
      "R2 Score: 0.2993\n",
      "\n",
      " Model Comparison:\n",
      "                Model            MSE        R2\n",
      "0  Linear Regression  128284.345497  0.290745\n",
      "1   Ridge Regression  126603.902644  0.300036\n",
      "2   Lasso Regression  126739.568991  0.299286\n",
      "\n",
      " Best Model:\n",
      "Model    Ridge Regression\n",
      "MSE         126603.902644\n",
      "R2               0.300036\n",
      "Name: 1, dtype: object\n",
      "\n",
      " Explanation:\n",
      "\n",
      "- Linear Regression: Fits the data directly, can overfit if features are highly correlated.\n",
      "- Ridge Regression: Adds L2 regularization (penalty on large coefficients), reduces overfitting.\n",
      "- Lasso Regression: Adds L1 regularization, can shrink some coefficients to zero (feature selection).\n",
      "\n",
      "Generally, Ridge performs better when there are many correlated features (multicollinearity),\n",
      "while Lasso is better when only a few features are truly important.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "file_path = \"Hitters.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Dataset Loaded Successfully\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "print(\"\\nMissing values before:\\n\", df.isnull().sum())\n",
    "df = df.dropna() \n",
    "print(\"\\nMissing values after cleaning:\\n\", df.isnull().sum())\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical Columns:\", list(categorical_cols))\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "X = df.drop(\"Salary\", axis=1)\n",
    "y = df[\"Salary\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test_scaled)\n",
    "\n",
    "ridge_model = Ridge(alpha=0.5748)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "lasso_model = Lasso(alpha=0.5748)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    return {\"Model\": model_name, \"MSE\": mse, \"R2\": r2}\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(y_test, y_pred_linear, \"Linear Regression\"))\n",
    "results.append(evaluate_model(y_test, y_pred_ridge, \"Ridge Regression\"))\n",
    "results.append(evaluate_model(y_test, y_pred_lasso, \"Lasso Regression\"))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Model Comparison:\\n\", results_df)\n",
    "\n",
    "best_model = results_df.loc[results_df[\"R2\"].idxmax()]\n",
    "print(\"\\n Best Model:\")\n",
    "print(best_model)\n",
    "print(\"\\n Explanation:\")\n",
    "print(\"\"\"\n",
    "- Linear Regression: Fits the data directly, can overfit if features are highly correlated.\n",
    "- Ridge Regression: Adds L2 regularization (penalty on large coefficients), reduces overfitting.\n",
    "- Lasso Regression: Adds L1 regularization, can shrink some coefficients to zero (feature selection).\n",
    "\n",
    "Generally, Ridge performs better when there are many correlated features (multicollinearity),\n",
    "while Lasso is better when only a few features are truly important.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090759a",
   "metadata": {},
   "source": [
    " MAM THE DATASET WHICH YOU HAVE PROVIDED FOR QUESTION 2 IS NOT OPENING..., WHEN I TRY TO OPEN.,IT SHOWS THAT THE PAGE IS NOT FOUND.., SO FOR QUESTION 2, I AM USING MY OWN DATASET WHICH I DOWNLOADED FROM THE KAGGLE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428e26",
   "metadata": {},
   "source": [
    "Q3 Cross Validation for Ridge and Lasso Regression  \n",
    "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV) function of Python. Implement both on Boston House Prediction Dataset (load_boston dataset from sklearn.datasets).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully\n",
      "Shape of X: (506, 13)\n",
      "Shape of y: (506,)\n",
      "\n",
      "RidgeCV Results\n",
      "Best Alpha: 2.976351441631316\n",
      "R² Score: 0.6678168873825823\n",
      "Mean Squared Error: 24.36024435020685\n",
      "\n",
      "LassoCV Results\n",
      "Best Alpha: 0.001\n",
      "R² Score: 0.6687104601341602\n",
      "Mean Squared Error: 24.294715279804656\n",
      "\n",
      "Lasso Regression performs better on this dataset.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "print(\"Dataset Loaded Successfully\")\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "alphas = np.logspace(-3, 3, 20) \n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "ridge_best_alpha = ridge_cv.alpha_\n",
    "ridge_pred = ridge_cv.predict(X_test)\n",
    "\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "print(\"RidgeCV Results\")\n",
    "print(\"Best Alpha:\", ridge_best_alpha)\n",
    "print(\"R² Score:\", ridge_r2)\n",
    "print(\"Mean Squared Error:\", ridge_mse)\n",
    "print()\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "lasso_best_alpha = lasso_cv.alpha_\n",
    "lasso_pred = lasso_cv.predict(X_test)\n",
    "\n",
    "lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "\n",
    "print(\"LassoCV Results\")\n",
    "print(\"Best Alpha:\", lasso_best_alpha)\n",
    "print(\"R² Score:\", lasso_r2)\n",
    "print(\"Mean Squared Error:\", lasso_mse)\n",
    "print()\n",
    "\n",
    "if ridge_r2 > lasso_r2:\n",
    "    print(\"Ridge Regression performs better on this dataset.\")\n",
    "else:\n",
    "    print(\"Lasso Regression performs better on this dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf051e6b",
   "metadata": {},
   "source": [
    "Q4 Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step) on Iris dataset using one vs. rest strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n",
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n",
      "Target classes: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Model Evaluation\n",
      "\n",
      "Accuracy: 0.9666666666666667\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      " Demonstrating One-vs-Rest Concept Manually:\n",
      "Trained classifier for class 'setosa'\n",
      "Coefficient shape: (1, 4)\n",
      "Intercept: [-2.41957815]\n",
      "\n",
      "Trained classifier for class 'versicolor'\n",
      "Coefficient shape: (1, 4)\n",
      "Intercept: [-0.87535907]\n",
      "\n",
      "Trained classifier for class 'virginica'\n",
      "Coefficient shape: (1, 4)\n",
      "Intercept: [-3.57467368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Target classes:\", iris.target_names)\n",
    "print()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "base_model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "ovr_model = OneVsRestClassifier(base_model)\n",
    "ovr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ovr_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation\")\n",
    "print()\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "print(\"\\n Demonstrating One-vs-Rest Concept Manually:\")\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    y_binary = (y_train == i).astype(int)\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "    clf.fit(X_train, y_binary)\n",
    "    print(f\"Trained classifier for class '{class_name}'\")\n",
    "    print(\"Coefficient shape:\", clf.coef_.shape)\n",
    "    print(\"Intercept:\", clf.intercept_)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
